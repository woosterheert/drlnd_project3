{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve Unity's 'Tennis' I have used a Deep Deterministic Policy Gradient (DDPG):\n",
    "- Both the value and the policy network consist in my implementation of two hidden layers of each 128 nodes, each with a relu activation.\n",
    "- Both networks take as input the combination of the local observations of the two rackets\n",
    "- The value network ends with a single node with no activation function to predict the value function.\n",
    "- The policy network ends with four nodes with a tanh activation to predict the actions for both players. \n",
    "- The outcome of the policy network is updated with noise drawn from a normal distribution. The std of this distribution is linearly decreased from 1 in the beginning to 0.01 at the end of training to balance exploration vs exploitation.\n",
    "- Once the replay buffer is sufficiently filled, the networks get updated 10 times every 10 iterations.\n",
    "- The target networks for both the value and policy networks are updated with soft update of the online network parameters. \n",
    "\n",
    "The exact configuration of the various hyperparameters can be found in the following configuration dictionary, which is also used in the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'lr': 0.001, 'alpha': 0.01, 'update_freq': 10, 'gamma': 0.99,\n",
    "          'tau': 0.001, 'max_nr_steps': 2000, 'n_agents': 2, 'nr_updates': 10, 'noise_scale_init': 1,\n",
    "         'nr_episodes': 2000, 'noise_scale_min': 0.01, 'noise_decl': 0.995, \n",
    "         'output_freq': 100, 'action_boundaries': [-1, 1], 'buffer_size': 100000, 'batch_size': 256}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting agent is able to obtain a rolling average score of 0.5 over the past 100 episodes after a little more than 3000 episodes, as can be seen in the following plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My current implementation does not make use of the fact that there is mirror symmetry in the actions of the rackets with respect to the speed and location of the ball. Immediate future work on my end would include investigating if I can speed up the training process by making use of this mirror symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
